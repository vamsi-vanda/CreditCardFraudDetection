{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training set rows: 170885\n",
      "Time to train model: 7.214 seconds\n"
     ]
    }
   ],
   "source": [
    "#Loading libraries\n",
    "from pyspark import SparkContext         #used for spark context\n",
    "from pyspark.sql import SQLContext       #used for creating dataframe\n",
    "import pandas as pd                      #used for loading csv\n",
    "\n",
    "#libraries used for creating vector and label points\n",
    "# Convert the data frame to a dense vector\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "#Mllib library for random forest\n",
    "from pyspark.mllib.tree import RandomForest,RandomForestModel\n",
    "from time import *\n",
    "\n",
    "\n",
    "sc = SparkContext('local','Credit Fraud')  # if using locally\n",
    "sql_sc = SQLContext(sc)\n",
    "pandas_df = pd.read_csv('~/cdacproject/Traindata.csv')  # assuming the file contains a header\n",
    "s_df = sql_sc.createDataFrame(pandas_df)\n",
    "\n",
    "# The last column contains the classification outcome. Turn this into an RDD\n",
    "# of Labeled Points.\n",
    "transformed_df = s_df.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "print(\"Number of training set rows: %d\" % transformed_df.count())\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Train our random forest model.\n",
    "model = RandomForest.trainClassifier(transformed_df, numClasses=2, categoricalFeaturesInfo={}, \\\n",
    "    numTrees=3, featureSubsetStrategy=\"auto\", impurity=\"gini\", \\\n",
    "    maxDepth=4, maxBins=32, seed=1023)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)\n",
    "\n",
    "#model.save(sc, \"~/cdacproject/RFModel\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test set rows: 113922\n",
      "Model accuracy: 99.926%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df = pd.read_csv('~/cdacproject/Testdata.csv')\n",
    "test_data = sql_sc.createDataFrame(test_df)\n",
    "\n",
    "# The last column contains the classification outcome. Turn this into an RDD\n",
    "# of Labeled Points.\n",
    "testdata = test_data.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "print(\"Number of test set rows: %d\" % testdata.count())\n",
    "\n",
    "#Loading the saved model\n",
    "saveModel = RandomForestModel.load(sc,\"~/cdacproject/RFModel\")\n",
    "\n",
    "# Make predictions and compute accuracy\n",
    "predictions = saveModel.predict(testdata.map(lambda x: x.features))\n",
    "labels_and_predictions = testdata.map(lambda x: x.label).zip(predictions)\n",
    "acc = labels_and_predictions.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "print(\"Model accuracy: %.3f%%\" % (acc * 100))\n",
    "\n",
    "sc.stop() #Stopping the spark context\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
